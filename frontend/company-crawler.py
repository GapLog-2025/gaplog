import json
import time
import re
import os
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


def clean_company_name(name):
    return re.sub(r'^(ì£¼ì‹íšŒì‚¬|\(ì£¼\))', '', name).strip()


def save_logo_image(logo_url, company_id):
    try:
        if logo_url.startswith('//'):
            logo_url = 'https:' + logo_url
        ext = os.path.splitext(logo_url.split('?')[0])[1] or '.jpg'
        path = f"public/data/logos/{company_id}{ext}"
        os.makedirs(os.path.dirname(path), exist_ok=True)
        img_data = requests.get(logo_url).content
        with open(path, 'wb') as f:
            f.write(img_data)
        return path
    except Exception as e:
        print(f"[{company_id}] ë¡œê³  ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


def parse_company_page(html, target_ceo):
    soup = BeautifulSoup(html, 'html.parser')

    def get_field(field_name):
        th = soup.find('th', string=field_name)
        if not th:
            return None
        td = th.find_next_sibling('td')
        value = td.select_one('div.value')
        return value.get_text(strip=True) if value else None

    ceo_name = get_field("ëŒ€í‘œì")
    if ceo_name is None or target_ceo not in ceo_name:
        return None

    return {
        "employeeCount": get_field("ì‚¬ì›ìˆ˜"),
        "revenue": get_field("ë§¤ì¶œì•¡"),
        "website": get_field("í™ˆí˜ì´ì§€"),
        "logoUrl": soup.select_one("div.logo img")["src"] if soup.select_one("div.logo img") else None
    }


def search_and_scrape(driver, company):
    name = clean_company_name(company["coNm"])
    company_id = company["id"]
    expected_ceo = company["reperNm"]

    try:
        driver.get("https://www.jobkorea.co.kr")
        time.sleep(1)
        search_input = driver.find_element(By.ID, "stext")
        search_input.clear()
        search_input.send_keys(name)
        search_input.send_keys(Keys.ENTER)
        time.sleep(2)

        # ê¸°ì—…ì •ë³´ íƒ­ í´ë¦­
        tabs = driver.find_elements(By.CSS_SELECTOR, "div#tabType a")
        corp_tab = next((tab for tab in tabs if "ê¸°ì—…ì •ë³´" in tab.text), None)
        if corp_tab:
            corp_tab.click()
            time.sleep(2)
        else:
            return None

        # ì²« ë²ˆì§¸ ê²°ê³¼ í´ë¦­
        results = driver.find_elements(By.CSS_SELECTOR, ".post-list-info .coTit a")
        if not results:
            return None
        results[0].click()
        time.sleep(2)

        # ìƒì„¸ í˜ì´ì§€ íŒŒì‹±
        html = driver.page_source
        scraped = parse_company_page(html, expected_ceo)
        if not scraped:
            return None

        # ë¡œê³  ì €ì¥
        logo_path = save_logo_image(scraped["logoUrl"], company_id) if scraped["logoUrl"] else None
        return {
            "employeeCount": scraped["employeeCount"],
            "revenue": scraped["revenue"],
            "website": scraped["website"],
            "logoPath": logo_path,
        }

    except Exception as e:
        print(f"[{company_id}] ì—ëŸ¬: {e}")
        return None


def main():
    with open('public/data/companies.json', 'r', encoding='utf-8') as f:
        data = json.load(f)

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    for company in data["companies"]:
        if company.get("crawlStatus") == "success":
            continue

        print(f"[{company['id']}] ì²˜ë¦¬ì¤‘: {company['coNm']}")
        result = search_and_scrape(driver, company)

        if result:
            company.update(result)
            company["crawlStatus"] = "success"
            print(f"  â®• ì™„ë£Œ")
        else:
            company["crawlStatus"] = "not_found"
            print(f"  â®• ì‹¤íŒ¨ ë˜ëŠ” ì •ë³´ ì—†ìŒ")

        time.sleep(2)

    driver.quit()

    with open('public/data/companies.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print("ğŸ”„ ëª¨ë“  ê¸°ì—… ì—…ë°ì´íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()
